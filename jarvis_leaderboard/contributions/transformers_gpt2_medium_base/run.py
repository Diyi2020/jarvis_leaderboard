# -*- coding: utf-8 -*-
"""ChemNLP_TitleToAbstract.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/174giPxWo0w0ToSXjWJ2-LVokAO6Bf72S
"""
#
#!pip install transformers datasets evaluate rouge_score jarvis-tools

from transformers import Trainer, TrainingArguments
from datasets import load_dataset
import numpy as np
import torch
import math
from tqdm import tqdm
import time
import evaluate
from collections import defaultdict
from jarvis.db.jsonutils import dumpjson
import random
from transformers import AutoTokenizer
from transformers import AutoModelForCausalLM

random_seed = 123
torch.manual_seed(random_seed)
random.seed(0)
np.random.seed(random_seed)
torch.backends.cudnn.deterministic = True

rouge_score = evaluate.load("rouge")

# import torch
# torch.cuda.is_available = lambda : False

tqdm.pandas()
device = "cpu"
if torch.cuda.is_available():
    device = torch.device("cuda")

# prompt="Nonuniform superconductivity and Josephson effect in conical ferromagnet can be described as"
model_checkpoint = "gpt2-medium"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)
model = AutoModelForCausalLM.from_pretrained(model_checkpoint).to(device)
def generate_text(prompt="What is a superconductor?",max_new_tokens=250,model_checkpoint = "gpt2-medium"):
    # tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)
    # model = AutoModelForCausalLM.from_pretrained(model_checkpoint).to(device)
    inputs = tokenizer(prompt, return_tensors="pt")
    # model = AutoModelForCausalLM.from_pretrained(checkpoint)
    # outputs = model.generate(**inputs, do_sample=True)
    outputs = model.generate(
        **inputs, do_sample=True, max_new_tokens=max_new_tokens
    )
    out = tokenizer.batch_decode(outputs, skip_special_tokens=True)
    return (out[0].replace('\n',' '))

p = generate_text()

p

import os
if not os.path.exists('jarvis_leaderboard'):
  !git clone https://github.com/usnistgov/jarvis_leaderboard.git
os.chdir('jarvis_leaderboard')
!pip install -e .

!ls jarvis_leaderboard/benchmarks/AI/TextGen/arxiv_gen_text.json.zip

import json,zipfile
fname = 'jarvis_leaderboard/benchmarks/AI/TextGen/arxiv_gen_text.json.zip'
temp = 'arxiv_gen_text.json'
zp = zipfile.ZipFile(fname)
train_test = json.loads(zp.read(temp))

from jarvis.db.figshare import data
import pandas as pd
arxiv_summary = data('arxiv_summary')
df = pd.DataFrame(arxiv_summary)
df = df.drop_duplicates()

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# from tqdm import tqdm
# info = {}
# for i,j in tqdm(train_test['test'].items()):
#   title=df[df['id']==i]['text'].values[0]
#   actual_abstract =df[df['id']==i]['ctext'].values[0]
#   prompt = title+' can be described as '
#   #prompt = 'Write an abstract on the title '+ prompt
#   pred = generate_text(prompt)
#   info[i] = pred
#   #break

from jarvis.db.jsonutils import dumpjson
dumpjson(data=info,filename='textgen.json')

mem=[]
for i,j in info.items():
  info1={}
  info1['id']=str(i)
  info1['prediction']=j
  mem.append(info1)

import pandas as pd
dff=pd.DataFrame(mem)
dff

dff.to_csv('AI-TextGen-text-arxiv_gen-test-rouge.csv',index=False)

!zip AI-TextGen-text-arxiv_gen-test-rouge.csv.zip AI-TextGen-text-arxiv_gen-test-rouge.csv

