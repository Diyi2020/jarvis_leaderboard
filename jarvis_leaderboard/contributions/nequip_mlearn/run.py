# -*- coding: utf-8 -*-
"""Train_MLFF_NEQUIP.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/knc6/jarvis-tools-notebooks/blob/master/jarvis-tools-notebooks/Train_MLFF_NEQUIP.ipynb
"""

# install wandb
!pip install -q wandb

# install nequip
!pip install -q nequip==0.5.5 torch==1.11  jarvis-tools

# fix colab imports
import site
site.main()

# set to allow anonymous WandB
import os
os.environ["WANDB_ANONYMOUS"] = "must"

!wget https://figshare.com/ndownloader/files/40357663 -O mlearn.json.zip

import json,zipfile
mlearn = json.loads(
        zipfile.ZipFile("mlearn.json.zip").read(
            "mlearn.json"
        )
    )

# install allegro
!git clone --depth 1 https://github.com/mir-group/allegro.git
!pip install allegro/

import os
if not os.path.exists('jarvis_leaderboard'):
  !git clone https://github.com/usnistgov/jarvis_leaderboard.git
os.chdir('jarvis_leaderboard')
!pip install -e .

yam="""# !! PLEASE NOTE: `minimal.yaml` is meant as a _minimal_ example of a tiny, fast
#                 training that can be used to verify your nequip install,
#                 the syntax of your configuration edits, etc.
#                 These are NOT recommended hyperparameters for real applications!
#                 Please see `example.yaml` for a reasonable starting point.

# general
ase_args:
  format: extxyz
avg_num_neighbors: auto
batch_size: 2
chemical_symbol_to_type:
  Si: 0
dataset: ase
dataset_file_name: ./Si_data/sitraj.xyz

root: results/Si-tut
run_name: minimal
seed: 123
dataset_seed: 456

# network
num_basis: 8
r_max: 4.0
l_max: 2
parity: true
num_features: 16

# data set
# the keys used need to be stated at least once in key_mapping, npz_fixed_field_keys or npz_keys
# key_mapping is used to map the key in the npz file to the NequIP default values (see data/_key.py)
# all arrays are expected to have the shape of (nframe, natom, ?) except the fixed fields
# note that if your data set uses pbc, you need to also pass an array that maps to the nequip "pbc" key
#dataset: npz                                                                       # type of data set, can be npz or ase
#dataset_url: http://quantum-machine.org/gdml/data/npz/aspirin_ccsd.zip             # url to download the npz. optional
#dataset_file_name: ./benchmark_data/aspirin_ccsd-train.npz                         # path to data set file
#key_mapping:
#  z: atomic_numbers                                                                # atomic species, integers
#  E: total_energy                                                                  # total potential eneriges to train to
#  F: forces                                                                        # atomic forces to train to
#  R: pos                                                                           # raw atomic positions
#npz_fixed_field_keys:                                                              # fields that are repeated across different examples
#  - atomic_numbers

#chemical_symbols:
#  - H
#  - O
#  - C
# logging
wandb: false
# verbose: debug

# training
n_train: 214
n_val: 25
n_test: 25
batch_size: 1
validation_batch_size: 5
max_epochs: 100

# loss function
loss_coeffs: forces

# optimizer
optimizer_name: Adam
"""
with open('si.yaml','w') as f:
  f.write(yam)

import yaml

with open('si.yaml', 'r') as stream:
    tut = yaml.load(stream, Loader=yaml.Loader)

tut

# Commented out IPython magic to ensure Python compatibility.
# %%time
# import os,glob,sys,yaml,pprint
# import zipfile
# import json
# import pandas as pd
# from jarvis.db.figshare import data
# from jarvis.core.atoms import Atoms
# import numpy as np
# from nequip.data import AtomicData, Collater, dataset_from_config, register_fields, AtomicDataDict
# from nequip.data.transforms import TypeMapper
# import fileinput
# import torch
# os.chdir('/content')
# #torch.cuda.is_available = lambda : False
# elements = ["Si"] #["Ni", "Si", "Ge", "Mo", "Cu", "Li"]
# 
# 
# 
# 
# os.environ["WANDB_ANONYMOUS"] = "must"
# cmd = "wandb offline"
# os.system(cmd)
# 
# for element in elements:
#     os.chdir('/content')
#     cmd = "rm -r Si_data"
#     os.system(cmd)
#     folder = "Si_data"
#     if not os.path.exists(folder):
#         os.makedirs(folder)
#     benchmark_energies = (
#         "jarvis_leaderboard/jarvis_leaderboard/benchmarks/AI/MLFF/mlearn_"
#         + element
#         + "_energy.json.zip"
#     )
#     temp_energies = benchmark_energies.split("/")[-1].split(".zip")[0]
#     energies = json.loads(
#         zipfile.ZipFile(benchmark_energies).read(temp_energies)
#     )
#     train_ids = list(energies["train"].keys())
#     test_ids = list(energies["test"].keys())
# 
#     f = open("Si_data/sitraj.xyz", "w")
#     line = ""
#     for i in mlearn:
#         if i["jid"] in train_ids:
#             # print(i)
#             atoms = Atoms.from_dict(i["atoms"])
#             line += str(atoms.num_atoms) + "\n"
#             line += (
#                 "Lattice="
#                 + '"'
#                 + " ".join(map(str, (atoms.lattice_mat).flatten()))
#                 + '"'
#                 + " Properties=species:S:1:pos:R:3:forces:R:3 energy="
#                 + str(i["energy"])
#                 # + ' stress="'
#                 # + " ".join(map(str, np.array(i["stresses"]).flatten()))
#                 # + '"'
#                 + " free_energy="
#                 + str(i["energy"])
#                 + ' pbc="T T T"'
#                 + "\n"
#             )
#             for m, n, p in zip(
#                 atoms.elements, atoms.cart_coords, i["forces"]
#             ):
#                 line += (
#                     str(m)
#                     + " "
#                     + " ".join(map(str, n))
#                     + " "
#                     + " ".join(map(str, p))
#                     + "\n"
#                 )
#             # print(line)
#             f.write(line)
#     for i in mlearn:
#         if i["jid"] in test_ids:
#             # print(i)
#             atoms = Atoms.from_dict(i["atoms"])
#             line += str(atoms.num_atoms) + "\n"
#             line += (
#                 "Lattice="
#                 + '"'
#                 + " ".join(map(str, (atoms.lattice_mat).flatten()))
#                 + '"'
#                 + " Properties=species:S:1:pos:R:3:forces:R:3 energy="
#                 + str(i["energy"])
#                 # + ' stress="'
#                 # + " ".join(map(str, np.array(i["stresses"]).flatten()))
#                 # + '"'
#                 + " free_energy="
#                 + str(i["energy"])
#                 + ' pbc="T T T"'
#                 + "\n"
#             )
#             for m, n, p in zip(
#                 atoms.elements, atoms.cart_coords, i["forces"]
#             ):
#                 line += (
#                     str(m)
#                     + " "
#                     + " ".join(map(str, n))
#                     + " "
#                     + " ".join(map(str, p))
#                     + "\n"
#                 )
#             f.write(line)
#             # print(line)
#     for i in mlearn:
#         if i["jid"] in test_ids:
#             # print(i)
#             atoms = Atoms.from_dict(i["atoms"])
#             line += str(atoms.num_atoms) + "\n"
#             line += (
#                 "Lattice="
#                 + '"'
#                 + " ".join(map(str, (atoms.lattice_mat).flatten()))
#                 + '"'
#                 + " Properties=species:S:1:pos:R:3:forces:R:3 energy="
#                 + str(i["energy"])
#                 # + ' stress="'
#                 # + " ".join(map(str, np.array(i["stresses"]).flatten()))
#                 # + '"'
#                 + " free_energy="
#                 + str(i["energy"])
#                 + ' pbc="T T T"'
#                 + "\n"
#             )
#             for m, n, p in zip(
#                 atoms.elements, atoms.cart_coords, i["forces"]
#             ):
#                 line += (
#                     str(m)
#                     + " "
#                     + " ".join(map(str, n))
#                     + " "
#                     + " ".join(map(str, p))
#                     + "\n"
#                 )
#             # print(line)
#             f.write(line)
#     f.close()
#     pprint.pprint(tut)
#     cmd = "rm -rf ./results"
#     os.system(cmd)
#     yaml_f = element+'_config.yaml'
#     tmp="  "+element+": 0"
#     tut['chemical_symbol_to_type'] ={element: 0}
#     tut['n_train'] = len(train_ids)
#     tut['shuffle'] = False
#     tut['n_test'] = len(test_ids)
#     tut['n_val'] = len(test_ids)
#     tut['batch_size'] = 2
#     # tut['r_max'] = 6
#     # tut['lmax'] = 2
#     # tut['num_layers'] = 2
#     # tut['env_embed_multiplicity'] = 64
#     # tut['two_body_latent_mlp_latent_dimensions']=[128, 256, 512, 1024]
#     # tut['latent_mlp_latent_dimension'] = [128] #[1024, 1024, 1024]
#     # tut['edge_eng_mlp_latent_dimension'] = 128
#     tut['learning_rate'] = 0.001
#     tut['train_val_split '] = 'sequential'
#     tut['shuffle']=False
#     #pprint.pprint('config',tut)
#     with open(yaml_f, "w+") as fp:
#         yaml.dump(tut,fp)
#     cmd = "nequip-train "+yaml_f
#     print('CMD',cmd)
#     os.system(cmd)
#     print('FINISHED')
#     import torch
#     from nequip.utils import Config
#     from nequip.model import model_from_config
#     from nequip.data import AtomicData, ASEDataset
# 
#     device = "cpu"
#     if torch.cuda.is_available():
#         device = torch.device("cuda")
#     config = Config.from_file(
#         "results/Si-tut/minimal/config.yaml"
#     )
# 
#     # config["train_on_keys"]=["forces", "total_energy"]
#     # config["model_builders"] = ["EnergyModel", "PerSpeciesRescale", "ForceOutput", "RescaleEnergyEtc"]
#     model = model_from_config(config, initialize=False)
#     d = torch.load(
#         "results/Si-tut/minimal/best_model.pth",
#         map_location=device,
#     )
#     model.load_state_dict(d)
# 
#     df = pd.DataFrame(mlearn)
# 
#     def get_allegro_forces(model=[], atoms=[], cutoff=5):
#         ase_atoms = atoms.ase_converter()
#         a = AtomicData.from_ase(ase_atoms, cutoff)
#         data = AtomicData.to_AtomicDataDict(a)
#         tm = TypeMapper(
#             chemical_symbol_to_type=config["chemical_symbol_to_type"]
#         )
#         data = tm(data)
#         out = model(data)
#         pen = (
#             out["total_energy"]
#             .squeeze()
#             .cpu()
#             .detach()
#             .numpy()
#             .tolist()
#         )
#         num_atoms = atoms.num_atoms
#         pf = out["forces"].squeeze().cpu().detach().numpy()
#         return pen, pf, 0
# 
# 
#     for i in glob.glob("jarvis_leaderboard/jarvis_leaderboard/benchmarks/AI/MLFF/*energy*.zip"):
# 
#         if "mlearn" in i and element in i:
#             fname_e = (
#                 "AI-MLFF-energy-"
#                 + i.split("/")[-1].split("_energy.json.zip")[0]
#                 + "-test-mae.csv"
#             )
#             fname_f = (
#                 "AI-MLFF-forces-"
#                 + i.split("/")[-1].split("_energy.json.zip")[0]
#                 + "-test-multimae.csv"
#             )
#             fname_s = (
#                 "AI-MLFF-stresses-"
#                 + i.split("/")[-1].split("_energy.json.zip")[0]
#                 + "-test-multimae.csv"
#             )
#             f_e = open(fname_e, "w")
#             f_f = open(fname_f, "w")
#             # f_s = open(fname_s, "w")
# 
#             f_e.write("id,target,prediction\n")
#             f_f.write("id,target,prediction\n")
#             # f_s.write("id,prediction\n")
#             #
#             print(i)
#             dat = json.loads(
#                 zipfile.ZipFile(i).read(
#                     i.split("/")[-1].split(".zip")[0]
#                 )
#             )
#             print(dat["test"])
#             for key, val in dat["test"].items():
#                 entry = df[df["jid"] == key]
#                 atoms = Atoms.from_dict(entry.atoms.values[0])
#                 # print(key,val,df[df['jid']==key],atoms)
#                 # energy,forces=get_alignn_forces(atoms)
#                 energy, forces, stress = get_allegro_forces(
#                     model=model, atoms=atoms
#                 )
#                 print(key, val, energy, atoms.num_atoms)
#                 line = (
#                     key
#                     + ","
#                     + str(entry.energy.values[0])
#                     + ","
#                     + str(energy)
#                     + "\n"
#                 )
#                 f_e.write(line)
#                 line = (
#                     key
#                     + ","
#                     + str(
#                         ";".join(
#                             map(
#                                 str,
#                                 np.array(
#                                     entry.forces.values[0]
#                                 ).flatten(),
#                             )
#                         )
#                     )
#                     + ","
#                     + str(
#                         ";".join(map(str, np.array(forces).flatten()))
#                     )
#                     + "\n"
#                 )
#                 f_f.write(line)
#                 # line = (
#                 #     key
#                 #     + ","
#                 #     + str(";".join(map(str, np.array(stress).flatten())))
#                 #     + "\n"
#                 # )
#                 # f_s.write(line)
#             f_e.close()
#             f_f.close()
#             # f_s.close()
#             zname = fname_e + ".zip"
#             with zipfile.ZipFile(zname, "w") as myzip:
#                 myzip.write(fname_e)
# 
#             zname = fname_f + ".zip"
#             with zipfile.ZipFile(zname, "w") as myzip:
#                 myzip.write(fname_f)
# 
#             # zname = fname_s + ".zip"
#             # with zipfile.ZipFile(zname, "w") as myzip:
#             #     myzip.write(fname_s)
#

yaml_f

import yaml,pprint
with open(yaml_f, 'r') as stream:
    data_loaded = yaml.load(stream, Loader=yaml.Loader)

pprint.pprint(data_loaded)

# Commented out IPython magic to ensure Python compatibility.
en_df = pd.read_csv('AI-MLFF-energy-mlearn_Si-test-mae.csv.zip')
from sklearn.metrics import mean_absolute_error
print(mean_absolute_error(en_df['target'],en_df['prediction']))
# %matplotlib inline
import matplotlib.pyplot as plt
plt.plot(en_df['target'],en_df['prediction'],'.')
plt.xlabel('DFT energy(eV)')
plt.ylabel('FF energy(eV)')

f_df = pd.read_csv('AI-MLFF-forces-mlearn_Si-test-multimae.csv.zip')
target = np.concatenate([np.array(i.split(';'),dtype='float') for i in f_df['target'].values])
pred= np.concatenate([np.array(i.split(';'),dtype='float') for i in f_df['prediction'].values])
print(mean_absolute_error(target,pred))
plt.plot(target,pred,'.')
plt.xlabel('DFT forces(eV/A)')
plt.ylabel('FF forces(eV/A)')









